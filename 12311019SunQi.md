## Q1

### Pt1.

#### Pretraining

- **目标**：
   在超大规模非标注语料上做语言建模（如Next Token Prediction），让模型学会通用的语言分布和世界知识。
- **获得的能力**：
  - 语法、语义理解与生成
  - 基本常识与世界知识
  - 一定程度的推理、翻译、摘要等“涌现”能力
- **主要挑战**：
  - 需要极大的数据和算力，训练成本高
  - 训练目标只管“像人类文本”，不保证真实、无害或有用，会产生幻觉、偏见、有害内容
  - 数据噪声大、分布多样，优化和稳定性有难度
- **代表模型**：GPT-3、PaLM、LLaMA 等基础大模型

#### Instruction Tuning

- **目标**：
   用人工整理的  数据对预训练模型做监督微调（SFT），使其更好地理解并遵循自然语言指令\。
- **获得的能力**：
  - 更好地“听人话”
  - 能在同一模型内泛化到多任务（问答、翻译、写代码等）
  - 格式更规范、回答更聚焦任务本身
- **主要挑战**：
  - 构造高质量、覆盖广、少模板化的指令数据集比较昂贵
  - 容易过拟合少量指令风格，导致回复刻板
  - 不同指令之间可能互相冲突，统一行为较难
- **代表模型**：InstructGPT、FLAN-T5、LLaMA-2-Chat（SFT 阶段）等

#### Alignment Tuning

- **目标**：
   通过人类偏好反馈（如 RLHF、规则驱动的宪法式 AI 等），让模型符合人类价值观和安全要求，兼顾有用（helpful），无害（harmless），诚实（honest）。
- **获得的能力**：
  - 主动拒绝违法、危险或明显有害的请求
  - 说话更礼貌、有边界，减少攻击性与偏见
  - 回答更贴近人类偏好：更清晰、结构化
- **主要挑战**：
  - 高质量偏好标注昂贵且带有主观性，不同群体价值观不一致
  - RLHF 中存在“奖励黑客”、训练不稳定等问题
  - 只能在有限标注数据上近似人类价值，仍可能出现对齐失效
- **代表模型**：ChatGPT（例如 GPT-3.5/4 经 RLHF）、Claude、Gemini 等对话安全模型

### Pt2.

#### Emergent Abilities

一些能力很可能只存在于大型模型中，而不存在于小型模型中。

#### Examples

博客提到了以下几种例子

+ **复杂推理**：如 GSM8K 数学题上，百亿参数以上模型配合链式思维提示，性能突然大幅超越传统微调模型，而且只需要少量带思维过程的示例。

+ **带知识的推理**：在问答和常识任务上，大模型可以不用外部检索，只依靠预训练内化的知识，通过 prompt 就达到接近或超过以往“检索 + 小模型”的系统。

+ **分布外鲁棒性**：在领域转移、加噪声、对抗扰动等分布外设置中，大模型的性能下降更小，某些情况下甚至超过原本在同分布上更强的小模型。

### Pt3.

#### Results

GSM8K + zero shot

```
gsm8k length==== 1319 , gsm8k acc==== 0.33510235026535257
```

5 shot

```
gsm8k length==== 1314 , gsm8k acc==== 0.16210045662100456
```